import logging
import os
import json
import sys

from vendor.pipeline import Pipeline
from db_generator.db_utils import generate_pony_orm_model

# initialize logging
logging.basicConfig(
    level = logging.INFO,
    format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers = [
        logging.FileHandler("container_execution.log"), logging.StreamHandler()
    ]
)

# Initialize the Database and Schema
logging.info('Generating Sqlite Database Schema')
generate_pony_orm_model()
# Import happens HERE because the functions being imported are generated by the above statement
sys.path.insert(0, '/code')
from data.gen.models import insert_Table_ats, select_statement, raw_query

# Initialize 'python container tool'
pipeline = Pipeline()

#####################################################################################################
# NOTHING ABOVE THIS LINE NEEDS TO BE TOUCHED FOR THE ASSESSMENT (Although nobody is stopping you!) #
#####################################################################################################

# Pipeline steps
@pipeline.task()
def part_one__extract_raw_data(path: str):
    """
    Extracts the raw data from csv, xlsx, json files
    Args:
        path: location of raw data
    Returns: Enitrely up to you! included is a dummy dataset, that passes through the rest of the pipeline
    """
    logging.info('Extracting Raw Data...')


    dummy = [
        {
            'time': '2022-04-05 10:00:00.0',
            'person_name': 'Arthur Applicant',
            'phone': '2134567890',
            'email': 'arthur.applicant@cox.net',
            'company': 'Acme Anvil Corporation',
            'role': 'Wile E. Coyote Revivor',
            'application_status': 'applied'
        },
        {
            'time': '2022-04-05 11:00:00.0',
            'person_name': 'Arthur Applicant',
            'phone': '2134567890',
            'email': 'arthur.applicant@cox.net',
            'company': 'Acme Anvil Corporation',
            'role': 'Wile E. Coyote Revivor',
            'application_status': 'disqualified'
        }
    ]
    return dummy

@pipeline.task(depends_on=part_one__extract_raw_data)
def part_one__transform_1(dataset):
    """
    Define a transformation here! One of [Quarantine, Normalization, Common Status Mapping]
    Args:
        dataset: Input Dataset (from extract, presumably)
    Returns: transformed dataset
    """
    logging.info('Performing Transform 1...')
    return dataset

@pipeline.task(depends_on=part_one__transform_1)
def part_one__transform_2(dataset):
    """
    Define a transformation here! One of [Quarantine, Normalization, Common Status Mapping]
    Args:
        dataset: Input Dataset (from first transform, presumably)
    Returns: transformed dataset
    """
    logging.info('Performing Transform 2...')
    return dataset

@pipeline.task(depends_on=part_one__transform_2)
def part_one__transform_3(dataset):
    """
    Define a transformation here! One of [Quarantine, Normalization, Common Status Mapping]
    Args:
        dataset: Input Dataset (from second transform, presumably)
    Returns: transformed dataset
    """
    logging.info('Performing Transform 3...')
    return dataset

@pipeline.task(depends_on=part_one__transform_3)
def load_data(dataset):
    """
    Loads the supplied Data into the table,
    Nothing needs to be done here as part of the assessment

    Args:
        dataset: data from the last task having a format as follows (also depicted in line 44): [
            {
                'time': '2022-04-05 11:00:00.0',
                'person_name': 'Arthur Applicant',
                'phone': '2134567890',
                'email': 'arthur.applicant@cox.net',
                'company': 'Acme Anvil Corporation',
                'role': 'Wile E. Coyote Revivor',
                'application_status': 'disqualified'
            }
        ]
        Note: that each dictionary will represent a record and the keys of the dictionary MUST match the keys above in order to be added to the database.

    Returns: None
    """
    # Load Data Into Table
    logging.info('Loading Data in SQLite Database...')
    insert_Table_ats(dataset)

@pipeline.task(depends_on=load_data)
def part_two_query_one(input: None):
    """
    Great place to write the first query of part 2!
    Args:
        input: None
    Returns: None
    """

    logging.info('Running First Query...')
    # Select Statement - This Throws SQL over PonyORM (https://docs.ponyorm.org/queries.html)
    query = """
    SELECT
    *
    FROM ats
    LIMIT 10
    """
    logging.info('Query:\n{}'.format(query))
    data = raw_query(query)
    logging.info('Results:\n{}'.format(
        json.dumps(data, indent = 2)
    ))

@pipeline.task(depends_on=part_two_query_one)
def part_two_query_two(input: None):
    """
    Great place to write the second query of part 2!
    Args:
        input: None
    Returns: None
    """

    logging.info('Running Second Query...')
    # Select Statement - This Throws SQL over PonyORM (https://docs.ponyorm.org/queries.html)
    query = """
    SELECT
    *
    FROM ats
    LIMIT 10
    """
    logging.info('Query:\n{}'.format(query))
    data = raw_query(query)
    logging.info('Results:\n{}'.format(
        json.dumps(data, indent = 2)
    ))


#####################################################################################################
# NOTHING BELOW THIS LINE NEEDS TO BE TOUCHED FOR THE ASSESSMENT (Although nobody is stopping you!) #
#####################################################################################################

def process():
    data_loc = 'input'
    pipeline.run(f"{data_loc}")

if __name__ == '__main__':
    process()